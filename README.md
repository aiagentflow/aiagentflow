# aiagentflow

A local-first CLI that orchestrates multi-agent AI workflows for software development. Give it a task â€” it coordinates specialized agents to architect, code, review, test, and ship automatically.

**No cloud dependency. Bring your own API keys. Your code stays on your machine.**

[![npm version](https://img.shields.io/npm/v/@aiagentflow/cli)](https://www.npmjs.com/package/@aiagentflow/cli)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Node.js](https://img.shields.io/badge/node-%3E%3D20-green)](https://nodejs.org)

---

## How It Works

```
Task â†’ Architect â†’ Coder â†’ Reviewer â†’ Tester â†’ Fixer â†’ Ship
```

Each stage uses a specialized AI agent with tuned prompts and parameters. The loop repeats until quality thresholds pass â€” like a small AI engineering team running on your machine.

---

## Install

```bash
npm install -g @aiagentflow/cli
```

Or with pnpm:

```bash
pnpm add -g @aiagentflow/cli
```

---

## Quick Start

```bash
# 1. Initialize in your project
cd /path/to/your/project
aiagentflow init

# 2. Run a task
aiagentflow run "Add a login form with email/password validation"

# 3. Or run autonomously (no approval prompts)
aiagentflow run "Refactor the auth module" --auto
```

The `init` wizard walks you through:
1. Select your LLM providers (Anthropic, Ollama)
2. Enter API keys
3. Assign models per agent role
4. Set workflow preferences

Configuration is saved locally in `.aiagentflow/config.json`.

---

## Features

- **Multi-agent pipeline** â€” 6 specialized agents, each with a distinct role
- **Local-first** â€” runs entirely on your machine, no code leaves your system
- **Provider-agnostic** â€” Anthropic (Claude), Ollama (local models), more coming
- **Configurable** â€” tune models, temperature, and iteration limits per agent
- **Git-native** â€” auto-creates branches for each task
- **Human-in-the-loop** â€” approve or override at any stage, or go full auto
- **QA policies** â€” configurable quality gates (max critical issues, test requirements)
- **Batch mode** â€” process multiple tasks from a file
- **Session persistence** â€” crash recovery with automatic session saving
- **Token tracking** â€” monitor LLM usage per agent and per run
- **Customizable prompts** â€” edit agent prompts in `.aiagentflow/prompts/`

---

## CLI Commands

| Command | Description |
|---------|-------------|
| `aiagentflow init` | Interactive setup wizard |
| `aiagentflow config` | View current configuration |
| `aiagentflow doctor` | Health check â€” verify providers and setup |
| `aiagentflow run <task>` | Run a workflow for a task |
| `aiagentflow run <task> --auto` | Autonomous mode (no approval prompts) |
| `aiagentflow run --batch tasks.txt` | Process multiple tasks from a file |

---

## Agent Roles

| Agent | Role | What it does |
|-------|------|-------------|
| ğŸ§  Architect | Plan | Analyzes the task and creates an implementation plan |
| ğŸ’» Coder | Implement | Writes production-ready code based on the plan |
| ğŸ” Reviewer | Review | Reviews code for bugs, security, and quality |
| ğŸ§ª Tester | Test | Generates tests and runs them |
| ğŸ› Fixer | Fix | Addresses review comments and test failures |
| âœ… Judge | QA | Final quality gate â€” pass or fail |

---

## Supported Providers

| Provider | Type | Setup |
|----------|------|-------|
| **Anthropic** | Cloud API | Requires `ANTHROPIC_API_KEY` |
| **Ollama** | Local | Requires [Ollama](https://ollama.com) running locally |

More providers (OpenAI, Groq, etc.) can be added by implementing a single adapter file.

### Using with Ollama (free, local)

```bash
# Install and start Ollama
ollama serve

# Pull a model
ollama pull llama3.2

# Initialize aiagentflow with Ollama
aiagentflow init
# â†’ Select "ollama" as provider
# â†’ Enter model name: llama3.2
```

---

## Configuration

After `aiagentflow init`, your project has:

```
.aiagentflow/
â”œâ”€â”€ config.json              # Main configuration
â”œâ”€â”€ prompts/                 # Customizable agent prompts
â”‚   â”œâ”€â”€ architect.md
â”‚   â”œâ”€â”€ coder.md
â”‚   â”œâ”€â”€ reviewer.md
â”‚   â”œâ”€â”€ tester.md
â”‚   â”œâ”€â”€ fixer.md
â”‚   â””â”€â”€ judge.md
â”œâ”€â”€ policies/                # Quality standards
â”‚   â””â”€â”€ coding-standards.md
â””â”€â”€ sessions/                # Saved workflow sessions
```

Edit the prompt files to customize how each agent behaves. Edit `coding-standards.md` to set project-specific rules that all agents follow.

---

## Project Structure

```
src/
â”œâ”€â”€ cli/            # CLI entry point and commands
â”œâ”€â”€ core/           # Config system, workflow engine, QA policies
â”œâ”€â”€ providers/      # LLM provider adapters (Anthropic, Ollama)
â”œâ”€â”€ agents/         # Agent implementations and prompt library
â”œâ”€â”€ git/            # Git operations wrapper
â”œâ”€â”€ prompts/        # Default prompt templates
â””â”€â”€ utils/          # Shared utilities (logger, fs, validation)
```

---

## Development

```bash
# Clone and install
git clone https://github.com/aiagentflow/aiagentflow.git
cd aiagentflow
pnpm install

# Run in dev mode
pnpm dev run "your task here"

# Type check
pnpm typecheck

# Run tests
pnpm test

# Lint & format
pnpm lint
pnpm format
```

---

## Contributing

Contributions are welcome! Here's how to get started:

1. **Fork** the repo and clone your fork
2. **Create a branch** for your feature: `git checkout -b feature/your-feature`
3. **Follow the coding standards:**
   - Functions: `camelCase`, Classes: `PascalCase`, Files: `kebab-case`
   - All public functions need JSDoc, types, and error handling
   - Use custom `AppError` subclasses â€” never raw `throw new Error()`
4. **Check your work:** `pnpm typecheck && pnpm lint && pnpm test`
5. **Open a PR** against `main` with a description of what and why

### Architecture rules

- Dependency direction flows downward: `cli â†’ core â†’ utils â†’ types`
- Config types are inferred from Zod schemas, never manually defined
- New providers only require one adapter file + registry entry

---

## Roadmap

- [x] Project scaffolding, config system, LLM provider layer
- [x] Workflow engine, agent implementations, Git integration
- [x] QA policies, token tracking, session persistence
- [ ] Context management for large repositories
- [ ] More providers (OpenAI, Groq, Mistral)
- [ ] VSCode extension
- [ ] Desktop GUI

---

## License

[MIT](LICENSE)

---

<p align="center">
  <a href="https://aiagentflow.dev">aiagentflow.dev</a>
</p>
